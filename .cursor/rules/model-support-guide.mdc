---
description: 
globs: 
alwaysApply: false
---
# AI Model Support and Integration Guide

This guide covers the comprehensive AI model support in Warden, including the latest models, reasoning capabilities, and integration patterns.

## Supported AI Providers and Models

### OpenAI Models
Configured in [AppConstants.swift](mdc:Warden/Configuration/AppConstants.swift):
- **Standard Models**: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
- **Reasoning Models**: o1, o1-preview, o1-mini, o3-mini, o3-mini-high, o3-mini-2025-01-31
- **Vision Support**: All gpt-4 variants support image input
- **Special Handling**: Reasoning models require system message conversion

### Anthropic (Claude) Models
- **Claude 3 Family**: claude-3-opus, claude-3-sonnet, claude-3-haiku
- **Claude 3.5**: claude-3-5-sonnet, claude-3-5-haiku
- **Vision Support**: All Claude 3+ models support image analysis
- **Context**: Large context windows (up to 200k tokens)

### Google (Gemini) Models
- **Gemini Pro**: gemini-pro, gemini-pro-vision
- **Gemini 2.0**: gemini-2.0-flash-exp
- **Gemini 2.5**: gemini-2.5-pro (latest model support)
- **Multimodal**: Native image and document processing

### DeepSeek Models
- **Chat Models**: deepseek-chat, deepseek-coder
- **Reasoning Models**: deepseek-reasoner (with thinking process support)
- **Special Features**: Reasoning content in separate stream

### xAI (Grok) Models
- **Grok Family**: grok-beta, grok-vision-beta
- **Grok 3**: grok-3-mini, grok-3 (latest additions)
- **Real-time Data**: Access to current information

### Perplexity Models
- **Standard**: llama-3.1-sonar-small, llama-3.1-sonar-large
- **Reasoning**: sonar-reasoning-pro, sonar-reasoning
- **Search Integration**: Built-in web search capabilities

### OpenRouter Models
- **Proxy Access**: 50+ models from various providers
- **Reasoning Models**: Access to o1, Claude, and other reasoning models
- **Unified API**: Single interface for multiple providers

### Groq Models
- **High-Speed Inference**: llama-3.1-70b-versatile, mixtral-8x7b
- **Performance Focus**: Optimized for speed over context length
- **Cost Effective**: Lower pricing for many use cases

### Mistral Models
- **Mistral Family**: mistral-large, mistral-medium, mistral-small
- **Codestral**: Specialized coding model
- **Multilingual**: Strong support for European languages

### Ollama (Local Models)
- **Local Processing**: Complete privacy with local inference
- **Model Management**: Download and manage models locally
- **Popular Models**: llama3, codellama, mistral, phi3
- **Apple Silicon**: Optimized for M1/M2/M3 Macs

## Model Detection and Configuration

### Reasoning Model Detection
Centrally managed in [AppConstants.swift](mdc:Warden/Configuration/AppConstants.swift):

```swift
static let openAiReasoningModels: Set<String> = [
    "o1", "o1-preview", "o1-mini", 
    "o3-mini", "o3-mini-high", "o3-mini-2025-01-31"
]

static let deepseekReasoningModels: Set<String> = [
    "deepseek-reasoner"
]

static let perplexityReasoningModels: Set<String> = [
    "sonar-reasoning-pro", "sonar-reasoning"
]
```

### Vision Model Detection
```swift
static let visionCapableModels: Set<String> = [
    "gpt-4o", "gpt-4-turbo", "gpt-4-vision-preview",
    "claude-3-opus", "claude-3-sonnet", "claude-3-haiku",
    "claude-3-5-sonnet", "claude-3-5-haiku",
    "gemini-pro-vision", "gemini-2.0-flash-exp", "gemini-2.5-pro",
    "grok-vision-beta"
]
```

### Model Capabilities Matrix
| Provider | Reasoning | Vision | Streaming | Local |
|----------|-----------|--------|-----------|-------|
| OpenAI | o1, o3 series | gpt-4 variants | ✅ | ❌ |
| Anthropic | ❌ | Claude 3+ | ✅ | ❌ |
| Google | ❌ | All Gemini | ✅ | ❌ |
| DeepSeek | deepseek-reasoner | ❌ | ✅ | ❌ |
| xAI | ❌ | Grok Vision | ✅ | ❌ |
| Perplexity | Reasoning models | ❌ | ✅ | ❌ |
| OpenRouter | Via proxy | Via proxy | ✅ | ❌ |
| Groq | ❌ | ❌ | ✅ | ❌ |
| Mistral | ❌ | ❌ | ✅ | ❌ |
| Ollama | ❌ | Some models | ✅ | ✅ |

## Integration Patterns

### Handler Selection
[APIServiceFactory.swift](mdc:Warden/Utilities/APIHandlers/APIServiceFactory.swift) manages handler creation:

```swift
static func createHandler(for service: String) -> APIProtocol? {
    switch service.lowercased() {
    case "openai": return ChatGPTHandler()
    case "anthropic": return ClaudeHandler()
    case "google": return GeminiHandler()
    case "deepseek": return DeepseekHandler()
    case "xai": return XAIHandler()
    case "perplexity": return PerplexityHandler()
    case "openrouter": return OpenRouterHandler()
    case "groq": return GroqHandler()
    case "mistral": return MistralHandler()
    case "ollama": return OllamaHandler()
    default: return nil
    }
}
```

### Reasoning Model Handling
Special processing for thinking-capable models:

1. **System Message Conversion**: Convert system role to user message
2. **Stream Processing**: Handle separate reasoning and content streams
3. **UI Integration**: Display thinking process in [ThinkingProcessView.swift](mdc:Warden/UI/Chat/ThinkingProcessView.swift)
4. **Content Formatting**: Wrap reasoning in `<think>` tags

### Multimodal Support
For vision-capable models:

1. **Image Processing**: [ImageAttachment.swift](mdc:Warden/Models/ImageAttachment.swift) handles image uploads
2. **Format Conversion**: Convert images to base64 for API transmission
3. **Request Building**: Include images in multipart requests
4. **Model Validation**: Ensure selected model supports vision

### Streaming Implementation
All handlers implement AsyncThrowingStream for real-time responses:

```swift
func sendMessage(...) async throws -> AsyncThrowingStream<String, Error> {
    return AsyncThrowingStream { continuation in
        // Handle streaming with proper cancellation
        // Special handling for reasoning content if applicable
    }
}
```

## Performance Considerations

### Model Selection Guidelines
- **Speed Priority**: Groq models for fastest inference
- **Quality Priority**: OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet
- **Reasoning Tasks**: o1/o3 series, deepseek-reasoner
- **Privacy**: Ollama local models
- **Cost Efficiency**: Smaller models like gpt-4o-mini, claude-3-haiku

### Context Management
- **Token Counting**: [TokenManager.swift](mdc:Warden/Utilities/TokenManager.swift) tracks context usage
- **Context Trimming**: Automatic conversation trimming for long chats
- **Smart Summarization**: Compress older messages to fit context limits
- **Model Limits**: Respect per-model context window limits

### Caching Strategy
- **Response Caching**: Cache appropriate responses for repeated queries
- **Model Metadata**: Cache model capabilities and pricing information
- **Image Processing**: Cache processed images to avoid re-encoding
- **Local Models**: Cache downloaded Ollama models

## Error Handling

### Model-Specific Errors
- **Rate Limits**: Different limits per provider and model
- **Context Overflow**: Handle context length exceeded errors
- **Model Availability**: Graceful fallback when models are unavailable
- **Authentication**: Provider-specific authentication error handling

### Reasoning Model Errors
- **Incomplete Reasoning**: Handle cut-off thinking processes
- **Format Errors**: Validate reasoning content structure
- **Stream Interruption**: Recover from broken reasoning streams
- **UI State**: Clean up thinking process display on errors

## Testing and Validation

### Model Testing
- **Response Quality**: Validate output quality across models
- **Performance Benchmarks**: Measure response times per model
- **Error Scenarios**: Test various failure modes
- **Reasoning Validation**: Verify thinking process display

### Integration Testing
- **Handler Switching**: Test switching between different providers
- **Multimodal Testing**: Validate image processing across vision models
- **Stream Testing**: Verify streaming performance and cancellation
- **Local Model Testing**: Test Ollama integration and model management

## Future Model Support

### Adding New Models
1. **Configuration**: Add model to [AppConstants.swift](mdc:Warden/Configuration/AppConstants.swift)
2. **Handler Update**: Update appropriate handler for new model
3. **Capability Detection**: Add to relevant capability sets
4. **Testing**: Validate integration and performance
5. **Documentation**: Update model support documentation

### Provider Integration
1. **Create Handler**: Implement [APIProtocol.swift](mdc:Warden/Utilities/APIHandlers/APIProtocol.swift)
2. **Factory Registration**: Add to [APIServiceFactory.swift](mdc:Warden/Utilities/APIHandlers/APIServiceFactory.swift)
3. **UI Integration**: Add provider to preferences and selection UI
4. **Authentication**: Implement provider-specific authentication
5. **Testing**: Comprehensive testing with mock and real APIs

## Best Practices

### Model Selection
- **Match Model to Task**: Use reasoning models for complex problems
- **Consider Context**: Choose models with appropriate context windows
- **Balance Cost**: Consider pricing for high-volume usage
- **Privacy Requirements**: Use local models for sensitive data

### Implementation
- **Capability Checking**: Always verify model capabilities before use
- **Graceful Degradation**: Fallback to alternative models when needed
- **Resource Management**: Proper cleanup of streams and connections
- **User Experience**: Clear feedback on model capabilities and limitations
